#!/usr/bin/env python3
"""
Bulk Company Overview Data Extractor for fin-trade-extract pipeline.

Features:
- Bulk processing of company overview data from Alpha Vantage
- Focuses ONLY on active common stocks (no ETFs, no delisted stocks)
- Intelligent incremental processing with watermark tracking
- Cross-exchange support for NASDAQ, NYSE, AMEX common stocks
- Rate limiting optimized for Alpha Vantage Premium (75 calls/minute)
- Comprehensive error handling and retry logic
- S3 upload with organized structure
- Snowflake integration for tracking processing status

This extractor is designed to process only companies that will have overview data,
avoiding wasted API calls on delisted stocks or ETFs that may not have overview records.
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any
import time
import requests
import boto3
from io import StringIO
import pandas as pd
import snowflake.connector

# Add parent directories to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))
from utils.incremental_etl import IncrementalETLManager, get_snowflake_config_from_env
from utils.symbol_screener import SymbolScreener, ScreeningCriteria, AssetType, ExchangeType

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f'/tmp/company_overview_bulk_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    ]
)
logger = logging.getLogger(__name__)


class CompanyOverviewExtractor:
    """Bulk company overview data extraction with incremental ETL intelligence."""

    def __init__(self):
        """Initialize extractor with configuration."""
        self.api_key = os.getenv('ALPHAVANTAGE_API_KEY')
        self.s3_bucket = os.getenv('S3_BUCKET', 'fin-trade-craft-landing')
        self.s3_prefix = os.getenv('S3_COMPANY_OVERVIEW_PREFIX', 'company_overview/')
        self.aws_region = os.getenv('AWS_REGION', 'us-east-1')
        self.load_date = os.getenv('LOAD_DATE', datetime.now().strftime('%Y%m%d_%H%M%S'))
        
        # Processing configuration - Overview only for active common stocks
        self.processing_mode = os.getenv('PROCESSING_MODE', 'incremental')
        self.universe_name = os.getenv('UNIVERSE_NAME', 'active_common_stocks')
        self.exchange_filter = os.getenv('EXCHANGE_FILTER', 'ALL')  # Will be filtered to major exchanges
        
        # Fixed filters for overview - only active common stocks
        self.asset_type_filter = 'Stock'  # Force to Stock only, no ETFs
        self.status_filter = 'Active'     # Force to Active only, no delisted
        
        # Rate limiting and batch configuration
        self.batch_size = int(os.getenv('BATCH_SIZE', '50'))  # Standard batch size for overview
        self.max_batches = int(os.getenv('MAX_BATCHES', '0')) if os.getenv('MAX_BATCHES') else None
        self.max_symbols = int(os.getenv('MAX_SYMBOLS', '0')) if os.getenv('MAX_SYMBOLS') else None
        self.failure_threshold = float(os.getenv('FAILURE_THRESHOLD', '0.5'))
        
        # Rate limiting (overview calls are less frequent than time series)
        self.api_delay = 0.8  # 75 calls/minute = 0.8s between calls
        self.retry_delay = 5.0
        self.max_retries = 3
        
        # Initialize services
        self.s3_client = boto3.client('s3', region_name=self.aws_region)
        self.snowflake_config = get_snowflake_config_from_env()
        
        # Statistics tracking
        self.stats = {
            'total_symbols': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'api_calls': 0,
            'start_time': datetime.now(),
            'errors': []
        }
        
        logger.info(f"Initialized Company Overview Extractor")
        logger.info(f"Processing mode: {self.processing_mode}")
        logger.info(f"Universe: {self.universe_name}")
        logger.info(f"Exchange filter: {self.exchange_filter}")
        logger.info(f"Asset type filter: {self.asset_type_filter} (FIXED - no ETFs)")
        logger.info(f"Status filter: {self.status_filter} (FIXED - no delisted)")
        logger.info(f"Batch size: {self.batch_size}")
        
        if not self.api_key:
            raise ValueError("ALPHAVANTAGE_API_KEY environment variable is required")

    def get_symbols_to_process(self) -> List[str]:
        """Get list of active common stock symbols that need overview data processing."""
        logger.info("üîç Getting symbols for company overview processing...")
        
        try:
            etl_manager = IncrementalETLManager(self.snowflake_config)
            
            if self.processing_mode == 'incremental':
                # Get active common stock symbols from the universe
                symbols = self._get_active_common_stock_symbols()
                
                if not symbols:
                    logger.warning("‚ö†Ô∏è No active common stock symbols found")
                    return []
                
                # Use incremental ETL logic to identify which ones need processing
                symbols_to_process = etl_manager.identify_symbols_to_process(
                    data_type='company_overview',
                    universe_symbols=symbols,
                    force_refresh=False,
                    max_symbols=self.max_symbols
                )
                
                logger.info(f"üìä Incremental processing: {len(symbols_to_process)} symbols need overview data")
                
            elif self.processing_mode == 'full_refresh':
                # Get all active common stock symbols regardless of processing status
                symbols_to_process = self._get_active_common_stock_symbols()
                if self.max_symbols:
                    symbols_to_process = symbols_to_process[:self.max_symbols]
                logger.info(f"üîÑ Full refresh: processing {len(symbols_to_process)} symbols")
                
            elif self.processing_mode == 'universe':
                # Process entire active common stock universe
                symbols_to_process = self._get_active_common_stock_symbols()
                if self.max_symbols:
                    symbols_to_process = symbols_to_process[:self.max_symbols]
                logger.info(f"üåê Universe processing: {len(symbols_to_process)} symbols")
                
            else:
                raise ValueError(f"Invalid processing_mode: {self.processing_mode}")
            
            etl_manager.close_connection()
            return symbols_to_process
            
        except Exception as e:
            logger.error(f"‚ùå Error getting symbols to process: {e}")
            raise

    def _get_active_common_stock_symbols(self) -> List[str]:
        """Get symbols for active common stocks only - no ETFs, no delisted stocks."""
        logger.info("üìã Fetching active common stock symbols...")
        
        try:
            conn = snowflake.connector.connect(**self.snowflake_config)
            cursor = conn.cursor()
            
            # Build query for active common stocks only
            where_clauses = [
                "status = 'Active'",               # Only active stocks
                "assetType = 'Stock'",             # Only common stocks, no ETFs
                "symbol IS NOT NULL", 
                "symbol != ''",
                "LEN(symbol) <= 5"                # Reasonable symbol length filter
            ]
            
            # Filter by exchange if specified
            if self.exchange_filter != 'ALL':
                where_clauses.append(f"exchange = '{self.exchange_filter}'")
            else:
                # Default to major exchanges for overview data
                where_clauses.append("exchange IN ('NASDAQ', 'NYSE', 'AMEX')")
            
            query = f"""
            SELECT DISTINCT symbol 
            FROM FIN_TRADE_EXTRACT.RAW.LISTING_STATUS
            WHERE {' AND '.join(where_clauses)}
            ORDER BY symbol
            """
            
            if self.max_symbols:
                query += f" LIMIT {self.max_symbols}"
            
            cursor.execute(query)
            results = cursor.fetchall()
            symbols = [row[0] for row in results]
            
            conn.close()
            
            logger.info(f"üìä Found {len(symbols)} active common stock symbols")
            if symbols:
                logger.info(f"üìù Sample symbols: {', '.join(symbols[:10])}" + 
                           (f" (and {len(symbols) - 10} more)" if len(symbols) > 10 else ""))
            
            return symbols
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching active common stock symbols: {e}")
            raise

    def fetch_company_overview(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Fetch company overview data for a single symbol."""
        url = "https://www.alphavantage.co/query"
        
        params = {
            'function': 'OVERVIEW',
            'symbol': symbol,
            'apikey': self.api_key
        }
        
        for attempt in range(self.max_retries):
            try:
                self.stats['api_calls'] += 1
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                # Check for API errors
                if 'Error Message' in data:
                    logger.warning(f"‚ùå API error for {symbol}: {data['Error Message']}")
                    return None
                    
                if 'Note' in data:
                    logger.warning(f"‚ö†Ô∏è Rate limit note for {symbol}: {data['Note']}")
                    time.sleep(self.retry_delay)
                    continue
                
                # Check for empty response (common for delisted stocks or ETFs)
                if not data or len(data) == 0 or data.get('Symbol') != symbol:
                    logger.info(f"üìù No overview data available for {symbol} (likely normal for this symbol type)")
                    return None
                
                # Log key fiscal fields for verification
                fiscal_year_end = data.get('FiscalYearEnd', 'N/A')
                latest_quarter = data.get('LatestQuarter', 'N/A')
                logger.debug(f"‚úÖ Successfully fetched overview data for {symbol} (FiscalYearEnd: {fiscal_year_end}, LatestQuarter: {latest_quarter})")
                return data
                
            except requests.exceptions.RequestException as e:
                logger.warning(f"‚ö†Ô∏è Request error for {symbol} (attempt {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    logger.error(f"‚ùå Failed to fetch {symbol} after {self.max_retries} attempts")
                    return None
            except Exception as e:
                logger.error(f"‚ùå Unexpected error fetching {symbol}: {e}")
                return None
        
        return None

    def process_overview_data(self, symbol: str, data: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """Process raw overview data into structured format."""
        try:
            # Add processing metadata
            processed_data = data.copy()
            processed_data['SYMBOL_ID'] = abs(hash(symbol)) % 1000000000
            processed_data['PROCESSED_DATE'] = datetime.now(timezone.utc).strftime('%Y-%m-%d')
            processed_data['LOAD_DATE'] = self.load_date
            
            # Ensure key fiscal fields are included (should already be in API response)
            fiscal_year_end = processed_data.get('FiscalYearEnd', '')
            latest_quarter = processed_data.get('LatestQuarter', '')
            
            logger.debug(f"üìä Processing {symbol}: FiscalYearEnd='{fiscal_year_end}', LatestQuarter='{latest_quarter}'")
            
            # Convert to DataFrame
            df = pd.DataFrame([processed_data])
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Error processing overview data for {symbol}: {e}")
            return None

    def upload_to_s3(self, df: pd.DataFrame, symbol: str) -> bool:
        """Upload processed overview data to S3."""
        try:
            # Convert DataFrame to CSV
            csv_buffer = StringIO()
            df.to_csv(csv_buffer, index=False)
            csv_content = csv_buffer.getvalue()
            
            # Create S3 key directly in company_overview folder (no date subfolders)
            s3_key = f"{self.s3_prefix}overview_{symbol}_{self.load_date}.csv"
            
            # Upload to S3
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=s3_key,
                Body=csv_content,
                ContentType='text/csv'
            )
            
            logger.debug(f"‚úÖ Uploaded {symbol} overview data to s3://{self.s3_bucket}/{s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to upload {symbol} to S3: {e}")
            return False

    def update_processing_status(self, symbol: str, success: bool, fiscal_date: str = None):
        """Update processing status in ETL watermarks table."""
        try:
            etl_manager = IncrementalETLManager(self.snowflake_config)
            etl_manager.update_processing_status(
                data_type='company_overview',
                symbol=symbol,
                success=success,
                fiscal_date=fiscal_date
            )
            etl_manager.close_connection()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not update processing status for {symbol}: {e}")

    def process_batch(self, symbols: List[str]) -> Dict[str, int]:
        """Process a batch of symbols."""
        batch_stats = {'successful': 0, 'failed': 0, 'skipped': 0}
        
        for i, symbol in enumerate(symbols):
            try:
                logger.info(f"üìä Processing {symbol} ({i + 1}/{len(symbols)})")
                
                # Rate limiting
                if i > 0:
                    time.sleep(self.api_delay)
                
                # Fetch overview data
                overview_data = self.fetch_company_overview(symbol)
                
                if overview_data is None:
                    batch_stats['skipped'] += 1
                    self.stats['skipped'] += 1
                    self.update_processing_status(symbol, success=False)
                    continue
                
                # Process the data
                df = self.process_overview_data(symbol, overview_data)
                if df is None:
                    batch_stats['failed'] += 1
                    self.stats['failed'] += 1
                    self.update_processing_status(symbol, success=False)
                    continue
                
                # Upload to S3
                s3_success = self.upload_to_s3(df, symbol)
                if s3_success:
                    batch_stats['successful'] += 1
                    self.stats['successful'] += 1
                    # Update processing status with the overview data date (or current date as fallback)
                    fiscal_date = overview_data.get('LatestQuarter', datetime.now().strftime('%Y-%m-%d'))
                    self.update_processing_status(symbol, success=True, fiscal_date=fiscal_date)
                    logger.info(f"‚úÖ Successfully processed and uploaded {symbol} to S3")
                else:
                    batch_stats['failed'] += 1
                    self.stats['failed'] += 1
                    self.update_processing_status(symbol, success=False)
                    logger.warning(f"‚ùå Failed to upload {symbol} to S3")
                    
            except Exception as e:
                logger.error(f"‚ùå Error processing {symbol}: {e}")
                batch_stats['failed'] += 1
                self.stats['failed'] += 1
                self.stats['errors'].append(f"{symbol}: {str(e)}")
                self.update_processing_status(symbol, success=False)
        
        return batch_stats

    def check_failure_threshold(self, batch_stats: Dict[str, int]) -> bool:
        """Check if failure rate exceeds threshold."""
        total_processed = batch_stats['successful'] + batch_stats['failed']
        if total_processed == 0:
            return False
        
        failure_rate = batch_stats['failed'] / total_processed
        if failure_rate > self.failure_threshold:
            logger.error(f"‚ùå Failure rate {failure_rate:.2%} exceeds threshold {self.failure_threshold:.2%}")
            return True
        return False

    def run_bulk_extraction(self):
        """Run the complete bulk company overview extraction process."""
        logger.info("üöÄ Starting bulk company overview extraction...")
        
        try:
            # Get symbols to process
            symbols = self.get_symbols_to_process()
            if not symbols:
                logger.warning("‚ö†Ô∏è No symbols found for processing")
                return
            
            self.stats['total_symbols'] = len(symbols)
            
            # Process in batches
            total_batches = (len(symbols) + self.batch_size - 1) // self.batch_size
            if self.max_batches:
                total_batches = min(total_batches, self.max_batches)
            
            logger.info(f"üìã Processing {len(symbols)} active common stock symbols in {total_batches} batches")
            logger.info(f"üéØ Focus: Active common stocks only (no ETFs, no delisted)")
            
            for batch_num in range(total_batches):
                start_idx = batch_num * self.batch_size
                end_idx = min(start_idx + self.batch_size, len(symbols))
                batch_symbols = symbols[start_idx:end_idx]
                
                logger.info(f"üîÑ Processing batch {batch_num + 1}/{total_batches}: {len(batch_symbols)} symbols")
                
                batch_stats = self.process_batch(batch_symbols)
                
                # Check failure threshold
                if self.check_failure_threshold(batch_stats):
                    logger.error("‚ùå Stopping extraction due to high failure rate")
                    break
                
                logger.info(f"üìä Batch {batch_num + 1} complete: ‚úÖ{batch_stats['successful']} ‚ö†Ô∏è{batch_stats['skipped']} ‚ùå{batch_stats['failed']}")
            
            # Final summary
            self.print_final_summary()
            
        except Exception as e:
            logger.error(f"‚ùå Critical error in bulk extraction: {e}")
            raise

    def print_final_summary(self):
        """Print final processing summary."""
        duration = datetime.now() - self.stats['start_time']
        
        logger.info("=" * 60)
        logger.info("üìã COMPANY OVERVIEW EXTRACTION SUMMARY")
        logger.info("=" * 60)
        logger.info(f"üîÑ Processing mode: {self.processing_mode}")
        logger.info(f"üåê Universe: {self.universe_name}")
        logger.info(f"üè¢ Exchange filter: {self.exchange_filter}")
        logger.info(f"üíº Asset type filter: {self.asset_type_filter} (FIXED)")
        logger.info(f"üéØ Status filter: {self.status_filter} (FIXED)")
        logger.info(f"üìä Total symbols: {self.stats['total_symbols']}")
        logger.info(f"üåê API calls made: {self.stats['api_calls']}")
        logger.info(f"‚úÖ Successful API + S3 uploads: {self.stats['successful']}")
        logger.info(f"‚ö†Ô∏è Skipped (no API data): {self.stats['skipped']}")
        logger.info(f"‚ùå Failed (API or S3 errors): {self.stats['failed']}")
        logger.info(f"‚è±Ô∏è Duration: {duration}")
        
        if self.stats['api_calls'] > 0:
            calls_per_minute = (self.stats['api_calls'] / duration.total_seconds()) * 60
            logger.info(f"üìà API rate: {calls_per_minute:.1f} calls/minute")
        
        # Additional S3/data insights
        success_rate = (self.stats['successful'] / self.stats['api_calls']) * 100 if self.stats['api_calls'] > 0 else 0
        logger.info(f"üìà Success rate: {success_rate:.1f}% (API calls that resulted in S3 files)")
        logger.info(f"üìÅ S3 location: s3://{self.s3_bucket}/{self.s3_prefix}")
        logger.info(f"üìù Expected files in Snowflake staging: {self.stats['successful']}")
        
        # Save results for workflow reporting
        results = {
            'processing_mode': self.processing_mode,
            'universe_name': self.universe_name,
            'exchange_filter': self.exchange_filter,
            'asset_type_filter': self.asset_type_filter,
            'status_filter': self.status_filter,
            'total_symbols': self.stats['total_symbols'],
            'successful': self.stats['successful'],
            'skipped': self.stats['skipped'],
            'failed': self.stats['failed'],
            'api_calls': self.stats['api_calls'],
            'duration_seconds': duration.total_seconds()
        }
        
        try:
            with open('/tmp/company_overview_results.json', 'w') as f:
                json.dump(results, f, indent=2)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not save results file: {e}")
        
        if self.stats['errors']:
            logger.warning(f"‚ö†Ô∏è Errors encountered during processing:")
            for error in self.stats['errors'][:10]:  # Show first 10 errors
                logger.warning(f"   {error}")
            if len(self.stats['errors']) > 10:
                logger.warning(f"   ... and {len(self.stats['errors']) - 10} more errors")
        
        logger.info("=" * 60)


def main():
    """Main execution function."""
    try:
        logger.info("üè¢ Starting Company Overview Bulk Extraction")
        logger.info("üéØ Focus: Active common stocks only (no ETFs, no delisted)")
        
        extractor = CompanyOverviewExtractor()
        extractor.run_bulk_extraction()
        
        logger.info("‚úÖ Company overview extraction completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Extraction failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()