name: Time Series ETL - Watermark Based
on:
  workflow_dispatch:
    inputs:
      exchange_filter:
        description: 'Exchange to process (NYSE, NASDAQ, AMEX, or ALL for all, ETF_AND_ALL_OTHER for ETF and everything except NASDAQ and NYSE)'
        required: false
        type: choice
        options:
          - ALL
          - NYSE
          - NASDAQ
          - AMEX
          - ETF_AND_ALL_OTHER
        default: ALL
      max_symbols:
        description: 'Maximum number of symbols to process (blank = all eligible symbols)'
        required: false
        type: number
      skip_recent_hours:
        description: 'Skip symbols processed within this many hours (blank = re-process all)'
        required: false
        type: number
      batch_size:
        description: 'Number of symbols to process per batch'
        required: false
        default: 50
        type: number

jobs:
  time-series-watermark-etl:
    runs-on: ubuntu-22.04
    timeout-minutes: 360  # 6 hours max
    permissions:
      id-token: write
      contents: read
    env:
      PYTHONUNBUFFERED: '1'
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Decode Snowflake private key
        run: echo "${{ secrets.SNOWFLAKE_PRIVATE_KEY_DER_B64 }}" | base64 -d > snowflake_rsa_key.der
        working-directory: ${{ github.workspace }}
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 requests snowflake-connector-python pandas
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-duration-seconds: 14400  # or 43200, but <= max session duration
      - name: Fetch time series data using watermarks
        working-directory: ${{ github.workspace }}
        env:
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: fin-trade-craft-landing
          S3_TIME_SERIES_PREFIX: time_series_daily_adjusted/
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          EXCHANGE_FILTER: ${{ inputs.exchange_filter }}
          MAX_SYMBOLS: ${{ inputs.max_symbols }}
          SKIP_RECENT_HOURS: ${{ inputs.skip_recent_hours }}
          BATCH_SIZE: ${{ inputs.batch_size }}
        run: |
          echo "üöÄ Starting watermark-based time series ETL..."
          if [ "$EXCHANGE_FILTER" != "ALL" ]; then
            echo "üè¢ Exchange filter: $EXCHANGE_FILTER"
          else
            echo "üåê Processing: All exchanges"
          fi
          if [ -n "$MAX_SYMBOLS" ]; then
            echo "üîí Symbol limit: $MAX_SYMBOLS symbols (testing mode)"
          else
            echo "üìä Processing: All eligible symbols from watermarks"
          fi
          if [ -n "$SKIP_RECENT_HOURS" ]; then
            echo "‚è≠Ô∏è  Skip recent: Symbols processed within $SKIP_RECENT_HOURS hours will be skipped"
          else
            echo "üîÑ Re-process: All eligible symbols will be processed"
          fi
          echo "üìã Batch size: $BATCH_SIZE symbols"
          echo ""
          echo "üìç Time Series Logic:"
          echo "  - Only active common stocks (ASSET_TYPE='Stock' AND STATUS='Active')"
          echo "  - Staleness check: Only fetch if last update > 1 day ago"
          echo "  - Update watermarks with last update timestamp"
          echo ""
          echo "Running Time Series ETL fetch script..."
          python scripts/etl/fetch_time_series_watermark.py

      - name: Load time series data into Snowflake
        working-directory: ${{ github.workspace }}
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        run: |
          echo "üóÑÔ∏è Loading time series data into Snowflake..."
          python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_time_series_from_s3.sql

      - name: Clean up private key
        run: rm -f snowflake_rsa_key.der

      - name: Upload processing results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: watermark-etl-results-${{ github.run_number }}
          path: /tmp/watermark_etl_results.json
          retention-days: 30

      - name: Summary Report
        if: always()
        run: |
          echo "üìä WATERMARK-BASED ETL PROCESSING SUMMARY"
          echo "=========================================="
          if [ -n "$EXCHANGE_FILTER" ]; then
            echo "üè¢ Exchange: $EXCHANGE_FILTER"
          else
            echo "üåê Exchange: All"
          fi
          echo ""
          if [ -f /tmp/watermark_etl_results.json ]; then
            python -c "
          import json
          with open('/tmp/watermark_etl_results.json', 'r') as f:
              results = json.load(f)
          print(f'üìà Total symbols processed: {results[\"total_symbols\"]}')
          print(f'‚úÖ Successful: {results[\"successful\"]} ({results[\"successful\"]/results[\"total_symbols\"]*100:.1f}%)')  
          print(f'‚ùå Failed: {results[\"failed\"]}')
          print(f'‚è±Ô∏è  Duration: {results[\"duration_minutes\"]:.1f} minutes')
          # Show mode breakdown
          full_mode = sum(1 for d in results['details'] if d.get('mode') == 'full' and d.get('status') == 'success')
          compact_mode = sum(1 for d in results['details'] if d.get('mode') == 'compact' and d.get('status') == 'success')
          print(f'')
          print(f'Processing Mode Breakdown:')
          print(f'  üîÑ Full refresh: {full_mode} symbols')
          print(f'  ‚ö° Compact update: {compact_mode} symbols')
          # Calculate efficiency
          if results['total_symbols'] > 0 and results['duration_minutes'] > 0:
              efficiency = results['successful'] / results['duration_minutes']
              print(f'')
              print(f'‚ö° Processing efficiency: {efficiency:.1f} symbols/minute')
          print(f'')
          print(f'‚úÖ Watermarks updated for {results[\"successful\"]} symbols')
          print(f'   - FIRST_FISCAL_DATE set (if NULL)')
          print(f'   - LAST_FISCAL_DATE updated to latest data')
          print(f'   - LAST_SUCCESSFUL_RUN = current timestamp')
          print(f'   - CONSECUTIVE_FAILURES reset to 0')
          # Show delisted symbols marked
          if results.get('delisted_marked', 0) > 0:
              print(f'')
              print(f'üîí Delisted symbols marked as API_ELIGIBLE=\"DEL\": {results[\"delisted_marked\"]}')
              print(f'   (Symbols with DELISTING_DATE that have been successfully extracted)')
          "
          else
            echo "‚ùå No results file found - check logs for errors"
          fi
