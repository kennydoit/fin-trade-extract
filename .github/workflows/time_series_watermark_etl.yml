name: Time Series ETL - Watermark Based

on:
  workflow_dispatch:
    inputs:
      exchange_filter:
        description: 'Exchange to process (NYSE, NASDAQ, or leave blank for all)'
        required: false
        type: choice
        options:
          - ''
          - NYSE
          - NASDAQ
          - AMEX
        default: ''
      max_symbols:
        description: 'Maximum number of symbols to process (blank = all eligible symbols)'
        required: false
        type: number
      staleness_days:
        description: 'Days before data is considered stale (triggers full refresh)'
        required: false
        type: number
        default: 5
      skip_recent_hours:
        description: 'Skip symbols processed within this many hours (blank = re-process all)'
        required: false
        type: number
      batch_size:
        description: 'Number of symbols to process per batch'
        required: false
        type: number
        default: 50

jobs:
  watermark-time-series-etl:
    runs-on: ubuntu-22.04
    timeout-minutes: 360  # 6 hours max
    permissions:
      id-token: write
      contents: read
    env:
      PYTHONUNBUFFERED: '1'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install boto3 requests snowflake-connector-python

    - name: Configure AWS credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Fetch time series data using watermarks
      env:
        ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        S3_BUCKET: fin-trade-craft-landing
        S3_TIME_SERIES_PREFIX: time_series_daily_adjusted/
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        EXCHANGE_FILTER: ${{ inputs.exchange_filter }}
        MAX_SYMBOLS: ${{ inputs.max_symbols }}
        STALENESS_DAYS: ${{ inputs.staleness_days || '5' }}
        SKIP_RECENT_HOURS: ${{ inputs.skip_recent_hours }}
        BATCH_SIZE: ${{ inputs.batch_size || '50' }}
      run: |
        echo "üöÄ Starting watermark-based time series ETL..."
        if [ -n "$EXCHANGE_FILTER" ]; then
          echo "üè¢ Exchange filter: $EXCHANGE_FILTER"
        else
          echo "üåê Processing: All exchanges"
        fi
        if [ -n "$MAX_SYMBOLS" ]; then
          echo "üîí Symbol limit: $MAX_SYMBOLS symbols (testing mode)"
        else
          echo "üìä Processing: All eligible symbols from watermarks"
        fi
        echo "‚è∞ Staleness threshold: $STALENESS_DAYS days"
        if [ -n "$SKIP_RECENT_HOURS" ]; then
          echo "‚è≠Ô∏è  Skip recent: Symbols processed within $SKIP_RECENT_HOURS hours will be skipped"
        else
          echo "üîÑ Re-process: All symbols will be re-processed (use compact mode for recent data)"
        fi
        echo "üìã Batch size: $BATCH_SIZE symbols"
        echo ""
        echo "üìç Watermark Logic:"
        echo "  - If FIRST_FISCAL_DATE is NULL ‚Üí Full refresh (all history)"
        echo "  - If LAST_FISCAL_DATE < $STALENESS_DAYS days ago ‚Üí Compact mode (last 100 days)"
        echo "  - If LAST_FISCAL_DATE >= $STALENESS_DAYS days ago ‚Üí Full refresh"
        echo ""
        
        python scripts/github_actions/fetch_time_series_watermark.py

    - name: Load time series data into Snowflake
      env:
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      run: |
        echo "üóÑÔ∏è Loading time series data into Snowflake..."
        python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_time_series_from_s3.sql

    - name: Upload processing results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: watermark-etl-results-${{ github.run_number }}
        path: /tmp/watermark_etl_results.json
        retention-days: 30

    - name: Summary Report
      if: always()
      run: |
        echo "üìä WATERMARK-BASED ETL PROCESSING SUMMARY"
        echo "=========================================="
        if [ -n "${{ inputs.exchange_filter }}" ]; then
          echo "üè¢ Exchange: ${{ inputs.exchange_filter }}"
        else
          echo "üåê Exchange: All"
        fi
        echo "‚è∞ Staleness: ${{ inputs.staleness_days || '5' }} days"
        echo ""
        if [ -f /tmp/watermark_etl_results.json ]; then
          python -c "
        import json
        with open('/tmp/watermark_etl_results.json', 'r') as f:
            results = json.load(f)
        print(f'üìà Total symbols processed: {results[\"total_symbols\"]}')
        print(f'‚úÖ Successful: {results[\"successful\"]} ({results[\"successful\"]/results[\"total_symbols\"]*100:.1f}%)')  
        print(f'‚ùå Failed: {results[\"failed\"]}')
        print(f'‚è±Ô∏è  Duration: {results[\"duration_minutes\"]:.1f} minutes')
        
        # Show mode breakdown
        full_mode = sum(1 for d in results['details'] if d.get('mode') == 'full' and d.get('status') == 'success')
        compact_mode = sum(1 for d in results['details'] if d.get('mode') == 'compact' and d.get('status') == 'success')
        
        print(f'')
        print(f'Processing Mode Breakdown:')
        print(f'  üîÑ Full refresh: {full_mode} symbols')
        print(f'  ‚ö° Compact update: {compact_mode} symbols')
        
        # Calculate efficiency
        if results['total_symbols'] > 0 and results['duration_minutes'] > 0:
            efficiency = results['successful'] / results['duration_minutes']
            print(f'')
            print(f'‚ö° Processing efficiency: {efficiency:.1f} symbols/minute')
        
        print(f'')
        print(f'‚úÖ Watermarks updated for {results[\"successful\"]} symbols')
        print(f'   - FIRST_FISCAL_DATE set (if NULL)')
        print(f'   - LAST_FISCAL_DATE updated to latest data')
        print(f'   - LAST_SUCCESSFUL_RUN = current timestamp')
        print(f'   - CONSECUTIVE_FAILURES reset to 0')
        
        # Show delisted symbols marked
        if results.get('delisted_marked', 0) > 0:
            print(f'')
            print(f'üîí Delisted symbols marked as API_ELIGIBLE=\"DEL\": {results[\"delisted_marked\"]}')
            print(f'   (Symbols with DELISTING_DATE that have been successfully extracted)')
        "
        else
          echo "‚ùå No results file found - check logs for errors"
        fi
