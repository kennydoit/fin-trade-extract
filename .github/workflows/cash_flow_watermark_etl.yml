name: Cash Flow ETL - Watermark Based

on:
  workflow_dispatch:
    inputs:
      exchange_filter:
        description: 'Exchange to process (NYSE, NASDAQ, or leave blank for all)'
        required: false
        type: choice
        options:
          - ''
          - NYSE
          - NASDAQ
          - AMEX
        default: ''
      max_symbols:
        description: 'Maximum number of symbols to process (blank = all eligible symbols)'
        required: false
        type: number
      skip_recent_hours:
        description: 'Skip symbols processed within this many hours (blank = re-process all)'
        required: false
        type: number
      batch_size:
        description: 'Number of symbols to process per batch'
        required: false
        type: number
        default: 50
      consecutive_failure_threshold:
        description: 'Omit symbols with how many consecutive failures?'
        required: false
        type: number
        default: 3
  schedule:
    # Run weekly on Sunday at 3 AM UTC (fundamentals don't change daily)
    - cron: '0 3 * * 0'

jobs:
  watermark-cash-flow-etl:
    runs-on: ubuntu-22.04
    timeout-minutes: 360  # 6 hours max
    permissions:
      id-token: write
      contents: read
    env:
      PYTHONUNBUFFERED: '1'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install boto3 requests snowflake-connector-python

    - name: Configure AWS credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Fetch cash flow data using watermarks
      env:
        ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        S3_BUCKET: fin-trade-craft-landing
        S3_CASH_FLOW_PREFIX: cash_flow/
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        EXCHANGE_FILTER: ${{ inputs.exchange_filter }}
        MAX_SYMBOLS: ${{ inputs.max_symbols }}
        SKIP_RECENT_HOURS: ${{ inputs.skip_recent_hours }}
        BATCH_SIZE: ${{ inputs.batch_size || '50' }}
        CONSECUTIVE_FAILURE_THRESHOLD: ${{ inputs.consecutive_failure_threshold || '3' }}
      run: |
        echo "ğŸš€ Starting watermark-based cash flow ETL..."
        if [ -n "$EXCHANGE_FILTER" ]; then
          echo "ğŸ¢ Exchange filter: $EXCHANGE_FILTER"
        else
          echo "ğŸŒ Processing: All exchanges"
        fi
        if [ -n "$MAX_SYMBOLS" ]; then
          echo "ğŸ”’ Symbol limit: $MAX_SYMBOLS symbols (testing mode)"
        else
          echo "ğŸ“Š Processing: All eligible symbols from watermarks"
        fi
        if [ -n "$SKIP_RECENT_HOURS" ]; then
          echo "â­ï¸  Skip recent: Symbols processed within $SKIP_RECENT_HOURS hours will be skipped"
        else
          echo "ğŸ”„ Re-process: All eligible symbols will be processed"
        fi
        echo "ğŸ“‹ Batch size: $BATCH_SIZE symbols"
        echo ""
        echo "ğŸ“ Cash Flow Logic:"
        echo "  - Only active common stocks (ASSET_TYPE='Stock' AND STATUS='Active')"
        echo "  - Annual + Quarterly reports from Alpha Vantage API"
        echo "  - Update watermarks with fiscal date range"
        echo "  - Mark delisted stocks as 'DEL' after successful extraction"
        echo ""
        
        python scripts/etl/fetch_cash_flow_watermark.py

    - name: Load cash flow data into Snowflake
      env:
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      run: |
        echo "ï¿½ï¸ Loading cash flow data into Snowflake..."
        python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_cash_flow_from_s3.sql

    - name: Upload processing results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: watermark-etl-results-${{ github.run_number }}
        path: /tmp/watermark_etl_results.json
        retention-days: 30

    - name: Summary Report
      if: always()
      run: |
        echo "ğŸ“Š WATERMARK-BASED ETL PROCESSING SUMMARY"
        echo "=========================================="
        if [ -n "${{ inputs.exchange_filter }}" ]; then
          echo "ğŸ¢ Exchange: ${{ inputs.exchange_filter }}"
        else
          echo "ğŸŒ Exchange: All"
        fi
        echo ""
        if [ -f /tmp/watermark_etl_results.json ]; then
          python -c "
        import json
        with open('/tmp/watermark_etl_results.json', 'r') as f:
            results = json.load(f)
        print(f'ğŸ“ˆ Total symbols processed: {results[\"total_symbols\"]}')
        print(f'âœ… Successful: {results[\"successful\"]} ({results[\"successful\"]/results[\"total_symbols\"]*100:.1f}%)')  
        print(f'âŒ Failed: {results[\"failed\"]}')
        print(f'â±ï¸  Duration: {results[\"duration_minutes\"]:.1f} minutes')
        
        # Calculate efficiency
        if results['total_symbols'] > 0 and results['duration_minutes'] > 0:
            efficiency = results['successful'] / results['duration_minutes']
            print(f'')
            print(f'âš¡ Processing efficiency: {efficiency:.1f} symbols/minute')
        
        print(f'')
        print(f'âœ… Watermarks updated for {results[\"successful\"]} symbols')
        print(f'   - FIRST_FISCAL_DATE set (if NULL)')
        print(f'   - LAST_FISCAL_DATE updated to latest data')
        print(f'   - LAST_SUCCESSFUL_RUN = current timestamp')
        print(f'   - CONSECUTIVE_FAILURES reset to 0')
        
        # Show delisted symbols marked
        if results.get('delisted_marked', 0) > 0:
            print(f'')
            print(f'ğŸ”’ Delisted symbols marked as API_ELIGIBLE=\"DEL\": {results[\"delisted_marked\"]}')
            print(f'   (Symbols with DELISTING_DATE that have been successfully extracted)')
        "
        else
          echo "âŒ No results file found - check logs for errors"
        fi
