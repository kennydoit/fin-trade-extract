name: Bulk Time Series ETL - NASDAQ

on:
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of symbols to process per batch'
        required: false
        type: number
        default: 50
      max_batches:
        description: 'Maximum number of batches to process (safety limit, blank = no limit)'
        required: false
        type: number
      failure_threshold:
        description: 'Stop if success rate falls below this percentage (0.5 = 50%)'
        required: false
        type: number
        default: 0.5
      exchange_filter:
        description: 'Exchange filter (e.g., NASDAQ, NYSE, or ALL)'
        required: false
        type: string
        default: 'NASDAQ'
  schedule:
    # Run weekly on Sundays at 2 AM EST for full NASDAQ refresh
    - cron: '0 7 * * 0'  # Sunday at 7 AM UTC

jobs:
  bulk-time-series-etl:
    runs-on: ubuntu-22.04
    timeout-minutes: 360  # 6 hours max
    permissions:
      id-token: write
      contents: read
    env:
      PYTHONUNBUFFERED: '1'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scripts/github_actions/requirements.txt

    - name: Configure AWS credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Bulk fetch time series data for NASDAQ symbols
      env:
        ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
        AWS_REGION: ${{ secrets.AWS_REGION }}
        S3_BUCKET: fin-trade-craft-landing
        S3_TIME_SERIES_PREFIX: time_series_daily_adjusted/
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        BATCH_SIZE: ${{ inputs.batch_size || '50' }}
        MAX_BATCHES: ${{ inputs.max_batches }}
        FAILURE_THRESHOLD: ${{ inputs.failure_threshold || '0.5' }}
        EXCHANGE_FILTER: ${{ inputs.exchange_filter || 'NASDAQ' }}
        LOAD_DATE: ${{ github.run_number }}${{ github.run_attempt }}
      run: |
        echo "üöÄ Starting bulk NASDAQ time series extraction..."
        echo "üìã Batch size: $BATCH_SIZE symbols per batch"
        if [ -n "$MAX_BATCHES" ]; then
          echo "üõ°Ô∏è Safety limit: Maximum $MAX_BATCHES batches"
        else
          echo "üöÄ Processing mode: All available batches"
        fi
        echo "‚ö†Ô∏è Failure threshold: $FAILURE_THRESHOLD"
        echo "üè¢ Exchange filter: $EXCHANGE_FILTER"
        echo "üìÖ Load date: $LOAD_DATE"
        
        python scripts/github_actions/fetch_time_series_bulk.py

    - name: Load bulk time series data into Snowflake
      env:
        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
        SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
        SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
        SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      run: |
        echo "üóÑÔ∏è Loading bulk time series data into Snowflake..."
        python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_time_series_from_s3.sql

    - name: Upload processing results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: bulk-processing-results-${{ github.run_number }}
        path: /tmp/bulk_processing_results.json
        retention-days: 30

    - name: Summary Report
      if: always()
      run: |
        echo "üìä BULK PROCESSING SUMMARY"
        echo "========================="
        if [ -f /tmp/bulk_processing_results.json ]; then
          python -c "
        import json
        with open('/tmp/bulk_processing_results.json', 'r') as f:
            results = json.load(f)
        print(f'üìà Total symbols processed: {results[\"total_symbols\"]}')
        print(f'‚úÖ Successful: {results[\"successful\"]} ({results[\"successful\"]/results[\"total_symbols\"]*100:.1f}%)')  
        print(f'‚ùå Failed: {results[\"failed\"]}')
        print(f'‚è±Ô∏è  Total processing time: {results[\"total_time_minutes\"]:.1f} minutes')
        print(f'üè¢ Ready for Snowflake: {results[\"successful\"]} symbol datasets')
        "
        else
          echo "‚ùå No results file found - check logs for errors"
        fi