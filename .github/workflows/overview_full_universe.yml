name: Company Overview ETL

on:
  workflow_dispatch:
    inputs:
      processing_mode:
        description: 'Processing mode (incremental, full_refresh, universe)'
        required: false
        default: 'incremental'
        type: choice
        options:
          - incremental
          - full_refresh
          - universe
      universe_name:
        description: 'Universe name for processing'
        required: false
        default: 'active_common_stocks'
        type: string
      exchange_filter:
        description: 'Exchange filter (ALL, NASDAQ, NYSE, AMEX)'
        required: false
        default: 'ALL'
        type: choice
        options:
          - ALL
          - NASDAQ
          - NYSE
          - AMEX
      batch_size:
        description: 'Number of symbols to process per batch'
        required: false
        default: '50'
        type: string
      max_batches:
        description: 'Maximum number of batches to process (for testing)'
        required: false
        type: string
      max_symbols:
        description: 'Maximum number of symbols to process (for testing)'
        required: false
        type: string
      failure_threshold:
        description: 'Failure threshold between 0.0 and 1.0'
        required: false
        default: '0.5'
        type: string
      load_date:
        description: 'Load date for Snowflake watermark (auto-generated if not specified)'
        required: false
        type: string
  schedule:
    # Weekly on Sundays at 6:00 AM UTC (overview data changes less frequently)
    - cron: '0 6 * * 0'

jobs:
  company-overview:
    runs-on: ubuntu-22.04
    permissions:
      id-token: write   # for AWS OIDC
      contents: read
    env:
      PYTHONUNBUFFERED: '1'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/github_actions/requirements.txt

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Fetch LISTING_STATUS and upload to S3
        env:
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: fin-trade-craft-landing
          S3_LISTING_STATUS_PREFIX: listing_status/
        run: |
          python scripts/github_actions/fetch_listing_status_to_s3.py

      - name: Load LISTING_STATUS into Snowflake
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          LOAD_DATE: ${{ github.event.inputs.load_date }}
        run: |
          python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_listing_status_from_s3_simple.sql

      - name: Bulk fetch company overview data with incremental ETL
        env:
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET: fin-trade-craft-landing
          S3_COMPANY_OVERVIEW_PREFIX: company_overview/
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          PROCESSING_MODE: ${{ inputs.processing_mode || 'incremental' }}
          UNIVERSE_NAME: ${{ inputs.universe_name || 'active_common_stocks' }}
          EXCHANGE_FILTER: ${{ inputs.exchange_filter || 'ALL' }}
          BATCH_SIZE: ${{ inputs.batch_size || '50' }}
          MAX_BATCHES: ${{ inputs.max_batches }}
          MAX_SYMBOLS: ${{ inputs.max_symbols }}
          FAILURE_THRESHOLD: ${{ inputs.failure_threshold || '0.5' }}
          LOAD_DATE: ${{ github.run_number }}${{ github.run_attempt }}
        run: |
          echo "ğŸ¢ Starting incremental company overview ETL extraction..."
          echo "ğŸ¯ Target: Active common stocks only (no ETFs, no delisted)"
          echo "ğŸ”„ Processing mode: $PROCESSING_MODE"
          echo "ğŸŒ Universe: $UNIVERSE_NAME"  
          echo "ğŸ¢ Exchange filter: $EXCHANGE_FILTER"
          echo "ğŸ“‹ Batch size: $BATCH_SIZE symbols per batch"
          if [ -n "$MAX_BATCHES" ]; then
            echo "ğŸ›¡ï¸ Safety limit: Maximum $MAX_BATCHES batches"
          else
            echo "ğŸš€ Processing mode: All available batches"
          fi
          if [ -n "$MAX_SYMBOLS" ]; then
            echo "ğŸ”’ Symbol limit: $MAX_SYMBOLS symbols (testing)"
          fi
          echo "âš ï¸ Failure threshold: $FAILURE_THRESHOLD"
          echo "ğŸ“… Load date: $LOAD_DATE"
          
          python scripts/github_actions/fetch_company_overview_bulk.py

      - name: Load bulk company overview data into Snowflake
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
        run: |
          echo "ğŸ—„ï¸ Loading bulk company overview data into Snowflake..."
          python scripts/github_actions/snowflake_run_sql_file.py snowflake/runbooks/load_company_overview_from_s3.sql

      - name: Upload processing results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: company-overview-results-${{ github.run_number }}
          path: /tmp/company_overview_results.json
          retention-days: 30

      - name: Summary Report
        if: always()
        run: |
          echo "ğŸ¢ COMPANY OVERVIEW INCREMENTAL ETL PROCESSING SUMMARY"
          echo "===================================================="
          echo "ğŸ”„ Processing Mode: $PROCESSING_MODE"
          echo "ğŸŒ Universe: $UNIVERSE_NAME"
          echo "ğŸ¯ Target: Active common stocks only"
          echo ""
          if [ -f /tmp/company_overview_results.json ]; then
            python -c "
          import json
          with open('/tmp/company_overview_results.json', 'r') as f:
              results = json.load(f)
          print(f'ğŸ“ˆ Total symbols processed: {results[\"total_symbols\"]}')
          print(f'âœ… Successful: {results[\"successful\"]}')
          print(f'âš ï¸ Skipped: {results[\"skipped\"]}')
          print(f'âŒ Failed: {results[\"failed\"]}')
          print(f'ğŸŒ API calls made: {results[\"api_calls\"]}')
          print(f'â±ï¸ Duration: {results[\"duration_seconds\"]:.1f} seconds')
          if results['api_calls'] > 0:
              rate = (results['api_calls'] / results['duration_seconds']) * 60
              print(f'ğŸ“Š API rate: {rate:.1f} calls/minute')
          "
          else
            echo "âš ï¸ No results file found"
          fi